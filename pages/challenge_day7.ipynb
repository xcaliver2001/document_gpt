{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xcali\\AppData\\Local\\Temp\\ipykernel_5688\\3622206279.py:81: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. See LangGraph documentation for more details: https://langchain-ai.github.io/langgraph/. Refer here for its pre-built ReAct agent: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "text() got an unexpected keyword argument 'max_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m wiki_result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtools[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_run(query)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Step 2: Search DuckDuckGo\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m ddg_result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Step 3: Scrape content from a URL (if found via DuckDuckGo)\u001b[39;00m\n\u001b[0;32m    113\u001b[0m scrape_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mDuckDuckGoSearchTool._run\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     42\u001b[0m     ddg \u001b[38;5;241m=\u001b[39m DuckDuckGoSearchAPIWrapper()\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mddg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xcali\\project_gpt\\env\\lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:97\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.run\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run query through DuckDuckGo and return concatenated results.\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     99\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddgs_news(query)\n",
      "File \u001b[1;32mc:\\Users\\xcali\\project_gpt\\env\\lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:64\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[1;34m(self, query, max_results)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mduckduckgo_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDGS\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[1;32m---> 64\u001b[0m     ddgs_gen \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ddgs_gen]\n",
      "\u001b[1;31mTypeError\u001b[0m: text() got an unexpected keyword argument 'max_results'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Type\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import wikipedia\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Tool to search Wikipedia\n",
    "class WikipediaSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The topic you want to search on Wikipedia.\")\n",
    "\n",
    "class WikipediaSearchTool(BaseTool):\n",
    "    name: str = \"WikipediaSearchTool\"  # Added type annotation\n",
    "    description: str = \"Search for information on Wikipedia.\"  # Added type annotation\n",
    "    args_schema: Type[WikipediaSearchToolArgsSchema] = WikipediaSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query: str) -> str:  # Added return type annotation\n",
    "        try:\n",
    "            summary = wikipedia.summary(query, sentences=5)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            return f\"Error while searching Wikipedia: {str(e)}\"\n",
    "        \n",
    "\n",
    "# Tool to search DuckDuckGo\n",
    "class DuckDuckGoSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The topic you want to search using DuckDuckGo.\")\n",
    "\n",
    "class DuckDuckGoSearchTool(BaseTool):\n",
    "    name: str = \"DuckDuckGoSearchTool\"\n",
    "    description: str = \"Search for information using DuckDuckGo.\"\n",
    "    args_schema: Type[DuckDuckGoSearchToolArgsSchema] = DuckDuckGoSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            ddg = DuckDuckGoSearchAPIWrapper()\n",
    "            results = ddg.results(query)  # 수정: .run()이 아닌 .results()를 사용합니다.\n",
    "            if results:\n",
    "                return \"\\n\".join(result[\"href\"] for result in results[:3])  # 상위 3개 링크 반환\n",
    "            else:\n",
    "                return \"No results found.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error while searching DuckDuckGo: {str(e)}\"\n",
    "\n",
    "# Tool to scrape website content\n",
    "class WebScraperToolArgsSchema(BaseModel):\n",
    "    url: str = Field(description=\"The URL of the website to scrape text content from.\")\n",
    "\n",
    "class WebScraperTool(BaseTool):\n",
    "    name: str = \"WebScraperTool\"\n",
    "    description: str = \"Scrape text content from a given URL.\"\n",
    "    args_schema: Type[WebScraperToolArgsSchema] = WebScraperToolArgsSchema\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text[:2000]\n",
    "        except Exception as e:\n",
    "            return f\"Error while scraping the website: {str(e)}\"\n",
    "\n",
    "# Tool to save research to a .txt file\n",
    "class SaveToFileToolArgsSchema(BaseModel):\n",
    "    content: str = Field(description=\"The content to save to a file.\")\n",
    "    filename: str = Field(description=\"The name of the file.\")\n",
    "\n",
    "class SaveToFileTool(BaseTool):\n",
    "    name: str = \"SaveToFileTool\"\n",
    "    description: str = \"Save research content to a .txt file.\"\n",
    "    args_schema: Type[SaveToFileToolArgsSchema] = SaveToFileToolArgsSchema\n",
    "\n",
    "    def _run(self, content: str, filename: str) -> str:\n",
    "        try:\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(content)\n",
    "            return f\"Content successfully saved to {filename}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error while saving to file: {str(e)}\"\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        WikipediaSearchTool(),\n",
    "        DuckDuckGoSearchTool(),\n",
    "        WebScraperTool(),\n",
    "        SaveToFileTool(),\n",
    "    ],\n",
    "    agent_kwargs={\n",
    "        \"system_message\": SystemMessage(\n",
    "            content=\"\"\"\n",
    "            You are a research assistant.\n",
    "            Your task is to search for information using Wikipedia and DuckDuckGo, scrape content from relevant websites, and save your findings to a .txt file.\n",
    "            Always ensure the information is accurate and relevant.\n",
    "            \"\"\"\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "query = \"Research about the XZ backdoor\"\n",
    "\n",
    "# Step 1: Search Wikipedia\n",
    "wiki_result = agent.tools[0]._run(query)\n",
    "\n",
    "# Step 2: Search DuckDuckGo\n",
    "ddg_result = agent.tools[1]._run(query)\n",
    "\n",
    "# Step 3: Scrape content from a URL (if found via DuckDuckGo)\n",
    "scrape_result = \"\"\n",
    "if \"http\" in ddg_result:\n",
    "    first_url = ddg_result.split(\"\\n\")[0]  # Assuming the first line contains a URL\n",
    "    scrape_result = agent.tools[2]._run(first_url)\n",
    "\n",
    "# Step 4: Combine results and save to file\n",
    "combined_content = f\"Wikipedia Result:\\n{wiki_result}\\n\\nDuckDuckGo Result:\\n{ddg_result}\\n\\nScraped Content:\\n{scrape_result}\"\n",
    "filename = \"research_XZ_backdoor.txt\"\n",
    "file_save_result = agent.tools[3]._run(content=combined_content, filename=filename)\n",
    "\n",
    "print(file_save_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
